[{"allReturnParams": ["children", "n_components", "n_leaves", "parents", "distances"], "libName": "sklearn.cluster", "methods": [], "notes": "", "funcName": "ward_tree", "allFuncParams": ["X", "connectivity", "n_clusters", "return_distance"], "funcDesc": "Ward clustering based on a Feature matrix.", "funcParamBody": "X : array, shape (n_samples, n_features) feature matrix  representing n_samples samples to be clustered connectivity : sparse matrix (optional). connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. The matrix is assumed to be symmetric and only the upper triangular half is used. Default is None, i.e, the Ward algorithm is unstructured. n_clusters : int (optional) Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. In this case, the complete tree is not computed, thus the children output is of limited use, and the parents output should rather be used. This option is valid only when specifying a connectivity matrix. return_distance : bool (optional) If True, return the distance between the clusters.", "funcReturnBody": "children : 2D array, shape (n_nodes-1, 2) The children of each non-leaf node. Values less than n_samples i greater than or equal to n_samples is a non-leaf node and has children children_[i - n_samples] . Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i n_components : int The number of connected components in the graph. n_leaves : int The number of leaves in the tree parents : 1D array, shape (n_nodes, ) or None The parent of each node. Only returned when a connectivity matrix is specified, elsewhere None is returned. distances : 1D array, shape (n_nodes-1, ) Only returned if return_distance is set to True (for compatibility). The distances between the centers of the nodes. distances[i] children[i, 1] and children[i, 2] . If the nodes refer to leaves of the tree, then distances[i] is their unweighted euclidean distance. Distances are updated in the following way (from scipy.hierarchy.linkage): The new entry is computed as follows, where is the newly joined cluster consisting of clusters and , is an unused cluster in the forest, , and is the cardinality of its argument. This is also known as the incremental algorithm."},
{"allReturnParams": ["labels"], "libName": "sklearn.cluster", "methods": [], "notes": "The graph should contain only one connect component, elsewhere the results make little sense. This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering. The graph should contain only one connect component, elsewhere the results make little sense. This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering.", "funcName": "spectral_clustering", "allFuncParams": ["affinity", "Must be symmetric", "n_clusters", "n_components", "eigen_solver", "random_state", "n_init", "eigen_tol", "assign_labels"], "funcDesc": "Apply clustering to a projection to the normalized laplacian.", "funcParamBody": "affinity : array-like or sparse matrix, shape: (n_samples, n_samples) The affinity matrix describing the relationship of the samples to embed. Must be symmetric . Possible examples: adjacency matrix of a graph, heat kernel of the pairwise distance matrix of the samples, symmetric k-nearest neighbours connectivity matrix of the samples. n_clusters : integer, optional Number of clusters to extract. n_components : integer, optional, default is n_clusters Number of eigen vectors to use for the spectral embedding eigen_solver : {None, arpack, lobpcg, or amg} The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities random_state : int, RandomState instance or None, optional, default: None A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == amg and by the K-Means initialization. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. eigen_tol : float, optional, default: 0.0 Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver. assign_labels : {kmeans, discretize}, default: kmeans The strategy to use to assign labels in the embedding space.  There are two ways to assign labels after the laplacian embedding.  k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the Multiclass spectral clustering paper referenced below for more details on the discretization approach.", "funcReturnBody": "labels : array of integers, shape: n_samples The labels of the clusters."},
{"allReturnParams": ["cluster_centers", "labels"], "libName": "sklearn.cluster", "methods": [], "notes": "For an example, see examples/cluster/plot_mean_shift.py .", "funcName": "mean_shift", "allFuncParams": ["X", "bandwidth", "seeds", "bin_seeding", "min_bin_freq", "cluster_all", "max_iter", "n_jobs"], "funcDesc": "Perform mean shift clustering of data using a flat kernel.", "funcParamBody": "X : array-like, shape=[n_samples, n_features] Input data. bandwidth : float, optional Kernel bandwidth. If bandwidth is not given, it is determined using a heuristic based on the median of all pairwise distances. This will take quadratic time in the number of samples. The sklearn.cluster.estimate_bandwidth function can be used to do this more efficiently. seeds : array-like, shape=[n_seeds, n_features] or None Point used as initial kernel locations. If None and bin_seeding=False, each data point is used as a seed. If None and bin_seeding=True, see bin_seeding. bin_seeding : boolean, default=False If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. Ignored if seeds argument is not None. min_bin_freq : int, default=1 To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. cluster_all : boolean, default True If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1. max_iter : int, default 300 Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. New in version 0.17: Parallel Execution using n_jobs .", "funcReturnBody": "cluster_centers : array, shape=[n_clusters, n_features] Coordinates of cluster centers. labels : array, shape=[n_samples] Cluster labels for each point."},
{"allReturnParams": ["centroid", "label", "inertia", "best_n_iter"], "libName": "sklearn.cluster", "methods": [], "notes": "", "funcName": "k_means", "allFuncParams": ["X", "n_clusters", "init", "precompute_distances", "n_init", "max_iter", "verbose", "tol", "random_state", "copy_x", "n_jobs", "algorithm", "return_n_iter"], "funcDesc": "K-means clustering algorithm.", "funcParamBody": "X : array-like or sparse matrix, shape (n_samples, n_features) The observations to cluster. n_clusters : int The number of clusters to form as well as the number of centroids to generate. init : {k-means++, random, or ndarray, or a callable}, optional Method for initialization, default to k-means++: k-means++ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. random: generate k centroids from a Gaussian with mean and variance estimated from the data. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. precompute_distances : {auto, True, False} Precompute distances (faster but takes more memory). auto : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. verbose : boolean, optional Verbosity mode. tol : float, optional The relative increment in the results before declaring convergence. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first.  If copy_x is True, then the original data is not modified.  If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : auto, full or elkan, default=auto K-means algorithm to use. The classical EM-style algorithm is full. The elkan variation is more efficient by using the triangle inequality, but currently doesnt support sparse data. auto chooses elkan for dense data and full for sparse data. return_n_iter : bool, optional Whether or not to return the number of iterations.", "funcReturnBody": "centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the ith observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). best_n_iter : int Number of iterations corresponding to the best results. Returned only if return_n_iter is set to True."},
{"allReturnParams": ["bandwidth"], "libName": "sklearn.cluster", "methods": [], "notes": "", "funcName": "estimate_bandwidth", "allFuncParams": ["X", "quantile", "n_samples", "random_state", "n_jobs"], "funcDesc": "Estimate the bandwidth to use with the mean-shift algorithm.", "funcParamBody": "X : array-like, shape=[n_samples, n_features] Input points. quantile : float, default 0.3 should be between [0, 1] 0.5 means that the median of all pairwise distances is used. n_samples : int, optional The number of samples to use. If not given, all samples are used. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores.", "funcReturnBody": "bandwidth : float The bandwidth parameter."},
{"allReturnParams": ["core_samples", "labels"], "libName": "sklearn.cluster", "methods": [], "notes": "For an example, see examples/cluster/plot_dbscan.py . This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). Sparse neighborhoods can be precomputed using NearestNeighbors.radius_neighbors_graph with mode='distance' . Ester, M., H. P. Kriegel, J. Sander, and X. Xu, A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996", "funcName": "dbscan", "allFuncParams": ["X", "eps", "min_samples", "metric", "metric_params", "algorithm", "leaf_size", "p", "sample_weight", "n_jobs"], "funcDesc": "Perform DBSCAN clustering from vector array or distance matrix.", "funcParamBody": "X : array or sparse (CSR) matrix of shape (n_samples, n_features), or             array of shape (n_samples, n_samples) A feature array, or array of distances between samples if metric='precomputed' . eps : float, optional The maximum distance between two samples for them to be considered as in the same neighborhood. min_samples : int, optional The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. If metric is precomputed, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only nonzero elements may be considered neighbors for DBSCAN. metric_params : dict, optional Additional keyword arguments for the metric function. New in version 0.19. algorithm : {auto, ball_tree, kd_tree, brute}, optional The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. p : float, optional The power of the Minkowski metric to be used to calculate distance between points. sample_weight : array, shape (n_samples,), optional Weight of each sample, such that a sample with a weight of at least min_samples is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1. n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores.", "funcReturnBody": "core_samples : array [n_core_samples] Indices of core samples. labels : array [n_samples] Cluster labels for each point.  Noisy samples are given the label -1."},
{"allReturnParams": ["cluster_centers_indices", "labels", "n_iter"], "libName": "sklearn.cluster", "methods": [], "notes": "For an example, see examples/cluster/plot_affinity_propagation.py . Brendan J. Frey and Delbert Dueck, Clustering by Passing Messages Between Data Points, Science Feb. 2007 For an example, see examples/cluster/plot_affinity_propagation.py . Brendan J. Frey and Delbert Dueck, Clustering by Passing Messages Between Data Points, Science Feb. 2007", "funcName": "affinity_propagation", "allFuncParams": ["S", "preference", "convergence_iter", "max_iter", "damping", "copy", "verbose", "return_n_iter"], "funcDesc": "Perform Affinity Propagation Clustering of data", "funcParamBody": "S : array-like, shape (n_samples, n_samples) Matrix of similarities between points preference : array-like, shape (n_samples,) or float, optional Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, i.e. of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities (resulting in a moderate number of clusters). For a smaller amount of clusters, this can be set to the minimum value of the similarities. convergence_iter : int, optional, default: 15 Number of iterations with no change in the number of estimated clusters that stops the convergence. max_iter : int, optional, default: 200 Maximum number of iterations damping : float, optional, default: 0.5 Damping factor between 0.5 and 1. copy : boolean, optional, default: True If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency verbose : boolean, optional, default: False The verbosity level return_n_iter : bool, default False Whether or not to return the number of iterations.", "funcReturnBody": "cluster_centers_indices : array, shape (n_clusters,) index of clusters centers labels : array, shape (n_samples,) cluster labels for each point n_iter : int number of iterations run. Returned only if return_n_iter is set to True."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=8, eigen_solver=None, random_state=None, n_init=10, gamma=1.0, affinity=rbf, n_neighbors=10, eigen_tol=0.0, assign_labels=kmeans, degree=3, coef0=1, kernel_params=None, n_jobs=1)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like or sparse matrix, shape (n_samples, n_features)   OR, if affinity==`precomputed`, a precomputed affinity matrix of shape (n_samples, n_samples)   y : Ignored ", "methodDesc": "Creates an affinity matrix for X using the selected affinity, then applies spectral clustering to this affinity matrix."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_clusters", "eigen_solver", "random_state", "n_init", "gamma", "affinity", "n_neighbors", "eigen_tol", "assign_labels", "degree", "coef0", "kernel_params", "n_jobs"], "notes": "If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel: Where delta is a free parameter representing the width of the Gaussian kernel. Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points. If the pyamg package is installed, it is used: this greatly speeds up computation. If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel: Where delta is a free parameter representing the width of the Gaussian kernel. Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points. If the pyamg package is installed, it is used: this greatly speeds up computation.", "funcName": "SpectralClustering", "allFuncAttributes": ["affinity_matrix_", "labels_ :"], "funcDesc": "Apply clustering to a projection to the normalized laplacian.", "funcParamBody": "n_clusters : integer, optional The dimension of the projection subspace. eigen_solver : {None, arpack, lobpcg, or amg} The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities random_state : int, RandomState instance or None, optional, default: None A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == amg and by the K-Means initialization.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. gamma : float, default=1.0 Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for affinity='nearest_neighbors' . affinity : string, array-like or callable, default rbf If a string, this may be one of nearest_neighbors, precomputed, rbf or one of the kernels supported by sklearn.metrics.pairwise_kernels . Only kernels that produce similarity scores (non-negative values that increase with similarity) should be used. This property is not checked by the clustering algorithm. n_neighbors : integer Number of neighbors to use when constructing the affinity matrix using the nearest neighbors method. Ignored for affinity='rbf' . eigen_tol : float, optional, default: 0.0 Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver. assign_labels : {kmeans, discretize}, default: kmeans The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. degree : float, default=3 Degree of the polynomial kernel. Ignored by other kernels. coef0 : float, default=1 Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels. kernel_params : dictionary of string to any, optional Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels. n_jobs : int, optional (default = 1) The number of parallel jobs to run. If -1 , then the number of jobs is set to the number of CPU cores.", "funcAttrBody": "affinity_matrix_ : array-like, shape (n_samples, n_samples) Affinity matrix used for clustering. Available only if after calling fit . labels_ : : Labels of each point"},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=1)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape=[n_samples, n_features]   Samples to cluster.   y : Ignored ", "methodDesc": "Perform clustering."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the closest cluster each sample in X belongs to.", "methodParamsBody": "X : {array-like, sparse matrix}, shape=[n_samples, n_features]   New data to predict.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["bandwidth", "seeds", "bin_seeding", "min_bin_freq", "cluster_all", "n_jobs"], "notes": "Scalability: Because this implementation uses a flat kernel and a Ball Tree to look up members of each kernel, the complexity will tend towards O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2). Scalability can be boosted by using fewer seeds, for example by using a higher value of min_bin_freq in the get_bin_seeds function. Note that the estimate_bandwidth function is much less scalable than the mean shift algorithm and will be the bottleneck if it is used. Dorin Comaniciu and Peter Meer, Mean Shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619. Scalability: Because this implementation uses a flat kernel and a Ball Tree to look up members of each kernel, the complexity will tend towards O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2). Scalability can be boosted by using fewer seeds, for example by using a higher value of min_bin_freq in the get_bin_seeds function. Note that the estimate_bandwidth function is much less scalable than the mean shift algorithm and will be the bottleneck if it is used. Dorin Comaniciu and Peter Meer, Mean Shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619.", "funcName": "MeanShift", "allFuncAttributes": ["cluster_centers_", "labels_ :"], "funcDesc": "Mean shift clustering using a flat kernel.", "funcParamBody": "bandwidth : float, optional Bandwidth used in the RBF kernel. If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below). seeds : array, shape=[n_samples, n_features], optional Seeds used to initialize kernels. If not set, the seeds are calculated by clustering.get_bin_seeds with bandwidth as the grid size and default values for other parameters. bin_seeding : boolean, optional If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None. min_bin_freq : int, optional To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. If not defined, set to 1. cluster_all : boolean, default True If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.", "funcAttrBody": "cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers. labels_ : : Labels of each point."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=8, init=k-means++, max_iter=100, batch_size=100, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Training instances to cluster.   y : Ignored ", "methodDesc": "Compute the centroids on X by chunking it into mini-batches."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X", "u"], "methodReturns": ["labels"], "methodDesc": "Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X).", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.   u : Ignored "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array, shape [n_samples, k]   X transformed in the new space.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Compute clustering and transform X to cluster-distance space. Equivalent to fit(X).transform(X), but more efficiently implemented.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.   y : Ignored "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X", "y"], "methodName": "partial_fit(X, y=None)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Coordinates of the data points to cluster.   y : Ignored ", "methodDesc": "Update k means estimate on a single mini-batch X."}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the closest cluster each sample in X belongs to. In the vector quantization literature, cluster_centers_ is called the code book and each value returned by predict is the index of the closest code in the code book.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to predict.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float   Opposite of the value of X on the K-means objective.  ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Opposite of the value of X on the K-means objective.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data.   y : Ignored "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape [n_samples, k]   X transformed in the new space.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Transform X to a cluster-distance space. In the new space, each dimension is the distance to the cluster centers.  Note that even if X is sparse, the array returned by transform will typically be dense.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.  "}], "allFuncParams": ["n_clusters", "init", "max_iter", "batch_size", "verbose", "compute_labels", "random_state", "tol", "max_no_improvement", "init_size", "n_init", "reassignment_ratio"], "notes": "See http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf See http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf", "funcName": "MiniBatchKMeans", "allFuncAttributes": ["cluster_centers_", "labels_ :", "inertia_"], "funcDesc": "Mini-Batch K-Means clustering", "funcParamBody": "n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. init : {k-means++, random or an ndarray}, default: k-means++ Method for initialization, defaults to k-means++: k-means++ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. random: choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. max_iter : int, optional Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics. batch_size : int, optional, default: 100 Size of the mini batches. verbose : boolean, optional Verbosity mode. compute_labels : boolean, default=True Compute label assignment and inertia for the complete dataset once the minibatch optimization has converged in fit. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . tol : float, default: 0.0 Control early stopping based on the relative center changes as measured by a smoothed, variance-normalized of the mean center squared position changes. This early stopping heuristics is closer to the one used for the batch variant of the algorithms but induces a slight computational and memory overhead over the inertia heuristic. To disable convergence detection based on normalized center change, set tol to 0.0 (default). max_no_improvement : int, default: 10 Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed inertia. To disable convergence detection based on inertia, set max_no_improvement to None. init_size : int, optional, default: 3 * batch_size Number of samples to randomly sample for speeding up the initialization (sometimes at the expense of accuracy): the only algorithm is initialized by running a batch KMeans on a random subset of the data. This needs to be larger than n_clusters. n_init : int, default=3 Number of random initializations that are tried. In contrast to KMeans, the algorithm is only run once, using the best of the n_init initializations as measured by inertia. reassignment_ratio : float, default: 0.01 Control the fraction of the maximum number of counts for a center to be reassigned. A higher value means that low count centers are more easily reassigned, which means that the model will take longer to converge, but should converge in a better clustering.", "funcAttrBody": "cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers labels_ : : Labels of each point (if compute_labels is set to True). inertia_ : float The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=8, init=k-means++, n_init=10, max_iter=300, tol=0.0001, precompute_distances=auto, verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm=auto)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Training instances to cluster.   y : Ignored ", "methodDesc": "Compute k-means clustering."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X", "u"], "methodReturns": ["labels"], "methodDesc": "Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X).", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.   u : Ignored "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array, shape [n_samples, k]   X transformed in the new space.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Compute clustering and transform X to cluster-distance space. Equivalent to fit(X).transform(X), but more efficiently implemented.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.   y : Ignored "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the closest cluster each sample in X belongs to. In the vector quantization literature, cluster_centers_ is called the code book and each value returned by predict is the index of the closest code in the code book.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to predict.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float   Opposite of the value of X on the K-means objective.  ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Opposite of the value of X on the K-means objective.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data.   y : Ignored "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape [n_samples, k]   X transformed in the new space.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Transform X to a cluster-distance space. In the new space, each dimension is the distance to the cluster centers.  Note that even if X is sparse, the array returned by transform will typically be dense.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.  "}], "allFuncParams": ["n_clusters", "init", "n_init", "max_iter", "tol", "precompute_distances", "verbose", "random_state", "copy_x", "n_jobs", "algorithm"], "notes": "The k-means problem is solved using Lloyds algorithm. The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, How slow is the k-means method? SoCG2006) In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. Thats why it can be useful to restart it several times. The k-means problem is solved using Lloyds algorithm. The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, How slow is the k-means method? SoCG2006) In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. Thats why it can be useful to restart it several times.", "funcName": "KMeans", "allFuncAttributes": ["cluster_centers_", "labels_ :", "inertia_"], "funcDesc": "K-Means clustering", "funcParamBody": "n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. init : {k-means++, random or an ndarray} Method for initialization, defaults to k-means++: k-means++ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. random: choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. n_init : int, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, default: 300 Maximum number of iterations of the k-means algorithm for a single run. tol : float, default: 1e-4 Relative tolerance with regards to inertia to declare convergence precompute_distances : {auto, True, False} Precompute distances (faster but takes more memory). auto : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances verbose : int, default 0 Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . copy_x : boolean, default True When pre-computing distances it is more numerically accurate to center the data first.  If copy_x is True, then the original data is not modified.  If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : auto, full or elkan, default=auto K-means algorithm to use. The classical EM-style algorithm is full. The elkan variation is more efficient by using the triangle inequality, but currently doesnt support sparse data. auto chooses elkan for dense data and full for sparse data.", "funcAttrBody": "cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers labels_ : : Labels of each point inertia_ : float Sum of squared distances of samples to their closest cluster center."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=2, affinity=euclidean, memory=None, connectivity=None, compute_full_tree=auto, linkage=ward, pooling_func=<function mean>)", "methodDesc": ""}, {"methodName": "fit(X, y=None, **params)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the hierarchical clustering on the data", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The data   y : Ignored "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(Xred)", "methodReturnsBody": "X : array, shape=[n_samples, n_features] or [n_features]   A vector of size n_samples with the values of Xred assigned to each of the cluster of samples.  ", "methodParams": ["Xred"], "methodReturns": ["X"], "methodDesc": "Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each group of features", "methodParamsBody": "Xred : array-like, shape=[n_samples, n_clusters] or [n_clusters,]   The values to be assigned to each cluster of samples  "}, {"methodName": "pooling_func(a, axis=None, dtype=None, out=None, keepdims=<class numpy._globals._NoValue>)", "methodReturnsBody": "m : ndarray, see dtype parameter above   If out=None , returns a new array containing the mean values, otherwise a reference to the output array is returned.  ", "methodParams": ["a", "axis", "dtype", "out", "keepdims"], "methodReturns": ["m"], "methodDesc": "Compute the arithmetic mean along the specified axis. Returns the average of the array elements.  The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs. Notes The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below).  Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. By default, float16 results are computed using float32 intermediates for extra precision. Examples In single precision, mean can be inaccurate: Computing the mean in float64 is more accurate:", "methodParamsBody": "a : array_like   Array containing numbers whose mean is desired. If a is not an array, a conversion is attempted.   axis : None or int or tuple of ints, optional   Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.   New in version 1.7.0.   If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.   dtype : data-type, optional   Type to use in computing the mean.  For integer inputs, the default is float64 ; for floating point inputs, it is the same as the input dtype.   out : ndarray, optional   Alternate output array in which to place the result.  The default is None ; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See doc.ufuncs for details.   keepdims : bool, optional   If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then keepdims will not be passed through to the mean method of sub-classes of ndarray , however any non-default value will be.  If the sub-classes sum method does not implement keepdims any exceptions will be raised.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "Y : array, shape = [n_samples, n_clusters] or [n_clusters]   The pooled values for each feature cluster.  ", "methodParams": ["X"], "methodReturns": ["Y"], "methodDesc": "Transform a new matrix using the built clustering", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] or [n_features]   A M by N array of M observations in N dimensions or a length M array of M one-dimensional observations.  "}], "allFuncParams": ["n_clusters", "affinity", "memory", "connectivity", "compute_full_tree", "linkage", "pooling_func"], "notes": "The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below).  Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. By default, float16 results are computed using float32 intermediates for extra precision. In single precision, mean can be inaccurate: Computing the mean in float64 is more accurate: The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below).  Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. By default, float16 results are computed using float32 intermediates for extra precision. In single precision, mean can be inaccurate: Computing the mean in float64 is more accurate:", "funcName": "FeatureAgglomeration", "allFuncAttributes": ["labels_", "n_leaves_", "n_components_", "children_"], "funcDesc": "Agglomerate features.", "funcParamBody": "n_clusters : int, default 2 The number of clusters to find. affinity : string or callable, default euclidean Metric used to compute the linkage. Can be euclidean, l1, l2, manhattan, cosine, or precomputed. If linkage is ward, only euclidean is accepted. memory : None, str or object with the joblib.Memory interface, optional Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory. connectivity : array-like or callable, optional Connectivity matrix. Defines for each feature the neighboring features following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured. compute_full_tree : bool or auto, optional, default auto Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of features. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. linkage : {ward, complete, average}, optional, default ward Which linkage criterion to use. The linkage criterion determines which distance to use between sets of features. The algorithm will merge the pairs of cluster that minimize this criterion. ward minimizes the variance of the clusters being merged. average uses the average of the distances of each feature of the two sets. complete or maximum linkage uses the maximum distances between all features of the two sets. pooling_func : callable, default np.mean This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument axis=1 , and reduce it to an array of size [M].", "funcAttrBody": "labels_ : array-like, (n_features,) cluster labels for each feature. n_leaves_ : int Number of leaves in the hierarchical tree. n_components_ : int The estimated number of connected components in the graph. children_ : array-like, shape (n_nodes-1, 2) The children of each non-leaf node. Values less than n_features i greater than or equal to n_features is a non-leaf node and has children children_[i - n_features] . Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_features + i"},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(eps=0.5, min_samples=5, metric=euclidean, metric_params=None, algorithm=auto, leaf_size=30, p=None, n_jobs=1)", "methodDesc": ""}, {"methodParams": ["X", "sample_weight", "y"], "methodName": "fit(X, y=None, sample_weight=None)", "methodParamsBody": "X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)   A feature array, or array of distances between samples if metric='precomputed' .   sample_weight : array, shape (n_samples,), optional   Weight of each sample, such that a sample with a weight of at least min_samples is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.   y : Ignored ", "methodDesc": "Perform DBSCAN clustering from features or distance matrix."}, {"methodName": "fit_predict(X, y=None, sample_weight=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X", "sample_weight", "y"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)   A feature array, or array of distances between samples if metric='precomputed' .   sample_weight : array, shape (n_samples,), optional   Weight of each sample, such that a sample with a weight of at least min_samples is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.   y : Ignored "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["eps", "min_samples", "metric", "metric_params", "algorithm", "leaf_size", "p", "n_jobs"], "notes": "For an example, see examples/cluster/plot_dbscan.py . This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). Sparse neighborhoods can be precomputed using NearestNeighbors.radius_neighbors_graph with mode='distance' . Ester, M., H. P. Kriegel, J. Sander, and X. Xu, A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996 For an example, see examples/cluster/plot_dbscan.py . This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). Sparse neighborhoods can be precomputed using NearestNeighbors.radius_neighbors_graph with mode='distance' . Ester, M., H. P. Kriegel, J. Sander, and X. Xu, A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996", "funcName": "DBSCAN", "allFuncAttributes": ["core_sample_indices_", "components_", "labels_"], "funcDesc": "Perform DBSCAN clustering from vector array or distance matrix.", "funcParamBody": "eps : float, optional The maximum distance between two samples for them to be considered as in the same neighborhood. min_samples : int, optional The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.calculate_distance for its metric parameter. If metric is precomputed, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only nonzero elements may be considered neighbors for DBSCAN. New in version 0.17: metric precomputed to accept precomputed sparse matrix. metric_params : dict, optional Additional keyword arguments for the metric function. New in version 0.19. algorithm : {auto, ball_tree, kd_tree, brute}, optional The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. p : float, optional The power of the Minkowski metric to be used to calculate distance between points. n_jobs : int, optional (default = 1) The number of parallel jobs to run. If -1 , then the number of jobs is set to the number of CPU cores.", "funcAttrBody": "core_sample_indices_ : array, shape = [n_core_samples] Indices of core samples. components_ : array, shape = [n_core_samples, n_features] Copy of each core sample found by training. labels_ : array, shape = [n_samples] Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Input data.   y : Ignored ", "methodDesc": "Build a CF Tree for the input data."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X", "y"], "methodName": "partial_fit(X=None, y=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features), None   Input data. If X is not provided, only the global clustering step is done.   y : Ignored ", "methodDesc": "Online learning. Prevents rebuilding of CFTree from scratch."}, {"methodName": "predict(X)", "methodReturnsBody": "labels : ndarray, shape(n_samples)   Labelled data.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict data using the centroids_ of subclusters. Avoid computation of the row norms of X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Input data.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_trans : {array-like, sparse matrix}, shape (n_samples, n_clusters)   Transformed data.  ", "methodParams": ["X"], "methodReturns": ["X_trans"], "methodDesc": "Transform X into subcluster centroids dimension. Each dimension represents the distance from the sample point to each cluster centroid.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Input data.  "}], "allFuncParams": ["threshold", "branching_factor", "n_clusters", "compute_labels", "copy"], "notes": "The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node. For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated. The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node. For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated.", "funcName": "Birch", "allFuncAttributes": ["root_", "dummy_leaf_", "subcluster_centers_", "subcluster_labels_", "labels_"], "funcDesc": "Implements the Birch clustering algorithm.", "funcParamBody": "threshold : float, default 0.5 The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa. branching_factor : int, default 50 Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes. n_clusters : int, instance of sklearn.cluster model, default 3 Number of clusters after the final clustering step, which treats the subclusters from the leaves as new samples. None : the final clustering step is not performed and the subclusters are returned as they are. sklearn.cluster Estimator : If a model is provided, the model is fit treating the subclusters as new samples and the initial data is mapped to the label of the closest subcluster. int : the model fit is AgglomerativeClustering with n_clusters set to be equal to the int. compute_labels : bool, default True Whether or not to compute labels for each fit. copy : bool, default True Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten.", "funcAttrBody": "root_ : _CFNode Root of the CFTree. dummy_leaf_ : _CFNode Start pointer to all the leaves. subcluster_centers_ : ndarray, Centroids of all subclusters read directly from the leaves. subcluster_labels_ : ndarray, Labels assigned to the centroids of the subclusters after they are clustered globally. labels_ : ndarray, shape (n_samples,) Array of labels assigned to the input data. if partial_fit is used instead of fit, they are assigned to the last batch of data."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=2, affinity=euclidean, memory=None, connectivity=None, compute_full_tree=auto, linkage=ward, pooling_func=<function mean>)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the hierarchical clustering on the data", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The samples a.k.a. observations.   y : Ignored "}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_clusters", "affinity", "memory", "connectivity", "compute_full_tree", "linkage", "pooling_func"], "notes": "", "funcName": "AgglomerativeClustering", "allFuncAttributes": ["labels_", "n_leaves_", "n_components_", "children_"], "funcDesc": "Agglomerative Clustering", "funcParamBody": "n_clusters : int, default=2 The number of clusters to find. affinity : string or callable, default: euclidean Metric used to compute the linkage. Can be euclidean, l1, l2, manhattan, cosine, or precomputed. If linkage is ward, only euclidean is accepted. memory : None, str or object with the joblib.Memory interface, optional Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory. connectivity : array-like or callable, optional Connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured. compute_full_tree : bool or auto (optional) Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. linkage : {ward, complete, average}, optional, default: ward Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. ward minimizes the variance of the clusters being merged. average uses the average of the distances of each observation of the two sets. complete or maximum linkage uses the maximum distances between all observations of the two sets. pooling_func : callable, default=np.mean This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument axis=1 , and reduce it to an array of size [M].", "funcAttrBody": "labels_ : array [n_samples] cluster labels for each point n_leaves_ : int Number of leaves in the hierarchical tree. n_components_ : int The estimated number of connected components in the graph. children_ : array-like, shape (n_nodes-1, 2) The children of each non-leaf node. Values less than n_samples i greater than or equal to n_samples is a non-leaf node and has children children_[i - n_samples] . Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i"},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity=euclidean, verbose=False)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)   Data matrix or, if affinity is precomputed , matrix of similarities / affinities.   y : Ignored ", "methodDesc": "Create affinity matrix from negative euclidean distances, then apply affinity propagation clustering."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape (n_samples,)   Index of the cluster each sample belongs to.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the closest cluster each sample in X belongs to.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   New data to predict.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["damping", "max_iter", "convergence_iter", "copy", "preference", "affinity", "verbose"], "notes": "For an example, see examples/cluster/plot_affinity_propagation.py . The algorithmic complexity of affinity propagation is quadratic in the number of points. Brendan J. Frey and Delbert Dueck, Clustering by Passing Messages Between Data Points, Science Feb. 2007 For an example, see examples/cluster/plot_affinity_propagation.py . The algorithmic complexity of affinity propagation is quadratic in the number of points. Brendan J. Frey and Delbert Dueck, Clustering by Passing Messages Between Data Points, Science Feb. 2007", "funcName": "AffinityPropagation", "allFuncAttributes": ["cluster_centers_indices_", "cluster_centers_", "labels_", "affinity_matrix_", "n_iter_"], "funcDesc": "Perform Affinity Propagation Clustering of data.", "funcParamBody": "damping : float, optional, default: 0.5 Damping factor (between 0.5 and 1) is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping). This in order to avoid numerical oscillations when updating these values (messages). max_iter : int, optional, default: 200 Maximum number of iterations. convergence_iter : int, optional, default: 15 Number of iterations with no change in the number of estimated clusters that stops the convergence. copy : boolean, optional, default: True Make a copy of input data. preference : array-like, shape (n_samples,) or float, optional Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, ie of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities. affinity : string, optional, default=``euclidean`` Which affinity to use. At the moment precomputed and euclidean are supported. euclidean uses the negative squared euclidean distance between points. verbose : boolean, optional, default: False Whether to be verbose.", "funcAttrBody": "cluster_centers_indices_ : array, shape (n_clusters,) Indices of cluster centers cluster_centers_ : array, shape (n_clusters, n_features) Cluster centers (if affinity != precomputed ). labels_ : array, shape (n_samples,) Labels of each point affinity_matrix_ : array, shape (n_samples, n_samples) Stores the affinity matrix used in fit . n_iter_ : int Number of iterations taken to converge."}]